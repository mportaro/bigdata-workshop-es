{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.add_jars('/app/postgresql-42.1.4.jar')\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Stocks:ETL\")\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_dir = '/dataset/stocks-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# UDF\n",
    "from pyspark.sql.types import StringType\n",
    "#\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(stocks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('filename', F.input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = spark.read.csv('/dataset/yahoo-symbols-201709.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symbol_from(filename):\n",
    "    return filename.split('/')[-1].split('.')[0].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'file:///dataset/stocks-small/ibm.us.txt' # => IBM\n",
    "extract_symbol_from('file:///dataset/stocks-small/ibm.us.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_symbol = F.udf(lambda filename: extract_symbol_from(filename), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_folder = stocks_dir\n",
    "df = spark.read \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .csv(stocks_folder) \\\n",
    "        .withColumn(\"name\", extract_symbol(F.input_file_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .csv(stocks_folder) \\\n",
    "        .withColumn(\"name\", extract_symbol(F.input_file_name())) \\\n",
    "        .withColumnRenamed(\"Date\", \"dateTime\") \\\n",
    "        .withColumnRenamed(\"Open\", \"open\") \\\n",
    "        .withColumnRenamed(\"High\", \"high\") \\\n",
    "        .withColumnRenamed(\"Low\", \"low\") \\\n",
    "        .withColumnRenamed(\"Close\", \"close\") \\\n",
    "        .drop(\"Volume\", \"OpenInt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_file = '/dataset/yahoo-symbols-201709.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_lookup = spark.read. \\\n",
    "        option(\"header\", True). \\\n",
    "        option(\"inferSchema\", True). \\\n",
    "        csv(lookup_file). \\\n",
    "        select(\"Ticker\", \"Category Name\"). \\\n",
    "        withColumnRenamed(\"Ticker\", \"symbol\"). \\\n",
    "        withColumnRenamed(\"Category Name\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.show(3)\n",
    "symbols_lookup.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_stocks \\\n",
    "    .withColumnRenamed('dateTime', \"full_date\") \\\n",
    "    .filter(\"full_date >= \\\"2017-09-01\\\"\") \\\n",
    "    .withColumn(\"year\", F.year(\"full_date\")) \\\n",
    "    .withColumn(\"month\", F.month(\"full_date\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"full_date\")) \\\n",
    "    .withColumnRenamed(\"name\", \"symbol\") \\\n",
    "    .join(symbols_lookup, [\"symbol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window20 = (Window.partitionBy(F.col('symbol')).orderBy(F.col(\"full_date\")).rowsBetween(-20, 0))\n",
    "window50 = (Window.partitionBy(F.col('symbol')).orderBy(F.col(\"full_date\")).rowsBetween(-50, 0))\n",
    "window100 = (Window.partitionBy(F.col('symbol')).orderBy(F.col(\"full_date\")).rowsBetween(-100, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_moving_avg_df = joined_df \\\n",
    "    .withColumn(\"ma20\", F.avg(\"close\").over(window20)) \\\n",
    "    .withColumn(\"ma50\", F.avg(\"close\").over(window50)) \\\n",
    "    .withColumn(\"ma100\", F.avg(\"close\").over(window100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Average\n",
    "stocks_moving_avg_df.select('symbol', 'close', 'ma20').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/dataset/output.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_moving_avg_df \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .parquet(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.createOrReplaceTempView(\"stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badHighestClosingPrice = spark.sql(\"SELECT symbol, MAX(close) AS price FROM stocks WHERE full_date >= '2017-09-01' AND full_date < '2017-10-01' GROUP BY symbol\")\n",
    "badHighestClosingPrice.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highestClosingPrice = spark.sql(\"SELECT symbol, MAX(close) AS price FROM stocks WHERE year=2017 AND month=9 GROUP BY symbol\")\n",
    "highestClosingPrice.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Postgres\n",
    "stocks_moving_avg_df \\\n",
    "    .drop(\"year\", \"month\", \"day\") \\\n",
    "    .write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/workshop\") \\\n",
    "    .option(\"dbtable\", \"workshop.stocks\") \\\n",
    "    .option(\"user\", \"workshop\") \\\n",
    "    .option(\"password\", \"w0rkzh0p\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
